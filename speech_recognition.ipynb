{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition\n",
    "The objective is to use the __Speech Commands Datasets__ to identify keywords like\n",
    "* yes\n",
    "* no\n",
    "* up\n",
    "* down\n",
    "* left\n",
    "* right\n",
    "* on\n",
    "* off\n",
    "* stop\n",
    "* go\n",
    "* Everything else is considered either *unknown* or *silence*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import all necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import scipy.io.wavfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRAIN_YES = './train/audio/yes/'\n",
    "TRAIN_NO = './train/audio/no/'\n",
    "TRAIN_UP = './train/audio/up/'\n",
    "TRAIN_DOWN = './train/audio/down/'\n",
    "TRAIN_LEFT = './train/audio/left/'\n",
    "TRAIN_RIGHT = './train/audio/right/'\n",
    "TRAIN_ON = './train/audio/on/'\n",
    "TRAIN_OFF = './train/audio/off/'\n",
    "TRAIN_STOP = './train/audio/stop/'\n",
    "TRAIN_GO = './train/audio/go/'\n",
    "\n",
    "# noise\n",
    "TRAIN_UNKNOWN = [\n",
    "    './train/audio/bird/', './train/audio/dog/', './train/audio/eight/', \n",
    "    './train/audio/four/', './train/audio/happy/', './train/audio/nine/',\n",
    "    './train/audio/one/', './train/audio/seven/', './train/audio/six/',\n",
    "    './train/audio/three/', './train/audio/two/', './train/audio/wow/',\n",
    "    './train/audio/zero/', './train/audio/bed/', './train/audio/cat/',\n",
    "    './train/audio/house/', './train/audio/marvin/', './train/audio/sheila/', \n",
    "    './train/audio/three/', './train/audio/_background_noise_/'\n",
    "]\n",
    "\n",
    "TEST_DIR = './test/audio/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read training/testing data from respective directives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list()\n",
    "labels = list()\n",
    "test = list()\n",
    "\n",
    "def read_data(train, labels, test, is_testing=False):\n",
    "    if is_testing == False:\n",
    "        print('Reading training data...')\n",
    "        \n",
    "        print('Reading sample audios for YES')\n",
    "        for audio in tqdm(os.listdir(TRAIN_YES)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_YES + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in YES with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[0] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for NO')\n",
    "        for audio in tqdm(os.listdir(TRAIN_NO)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_NO + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in NO with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[1] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for UP')\n",
    "        for audio in tqdm(os.listdir(TRAIN_UP)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_UP + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in UP with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[2] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for DOWN')\n",
    "        for audio in tqdm(os.listdir(TRAIN_DOWN)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_DOWN + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in DOWN with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[3] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for LEFT')\n",
    "        for audio in tqdm(os.listdir(TRAIN_LEFT)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_LEFT + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in LEFT with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[4] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for RIGHT')\n",
    "        for audio in tqdm(os.listdir(TRAIN_RIGHT)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_RIGHT + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in RIGHT with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[5] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for ON')\n",
    "        for audio in tqdm(os.listdir(TRAIN_ON)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_ON + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in ON with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[6] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for OFF')\n",
    "        for audio in tqdm(os.listdir(TRAIN_OFF)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_OFF + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in OFF with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[7] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for STOP')\n",
    "        for audio in tqdm(os.listdir(TRAIN_STOP)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_STOP + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in STOP with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[8] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        print('Reading sample audios for GO')\n",
    "        for audio in tqdm(os.listdir(TRAIN_GO)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TRAIN_GO + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                if clip.shape[0] > 16000:\n",
    "                    print('Found a clip in GO with shape more than 16000')\n",
    "                train.append(clip)\n",
    "                \n",
    "                label = np.zeros(11)\n",
    "                label[9] = 1\n",
    "                labels.append(label)\n",
    "                \n",
    "        \n",
    "        print('Reading sample audios for UNKNOWNS')\n",
    "        for d in TRAIN_UNKNOWN:\n",
    "            for audio in tqdm(os.listdir(d)):\n",
    "                ext = os.path.splitext(audio)[1]\n",
    "            \n",
    "                if ext is not 'md' or ext is not 'MD':\n",
    "                    \n",
    "                    try:\n",
    "                        rate, clip = scipy.io.wavfile.read(d + audio)\n",
    "                        if clip.shape[0] < 16000:\n",
    "                            clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                    \n",
    "                        if clip.shape[0] > 16000:                            \n",
    "                            print('Found a clip in UNKNOWN with shape more than 16000')\n",
    "                            print(clip.shape)\n",
    "                            continue\n",
    "                        train.append(clip)\n",
    "                    \n",
    "                        label = np.zeros(11)\n",
    "                        label[10] = 1\n",
    "                        labels.append(label)\n",
    "                        \n",
    "                    except:\n",
    "                        print(d + audio)\n",
    "        \n",
    "        train = np.array(train)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        np.save('train.npy', train)\n",
    "        np.save('labels.npy', labels)\n",
    "                    \n",
    "    else:\n",
    "        print('Reading testing data...')\n",
    "        df = pd.DataFrame(columns=['fname', 'label'])\n",
    "        i = 0\n",
    "        \n",
    "        for audio in tqdm(os.listdir(TEST_DIR)):\n",
    "            ext = os.path.splitext(audio)[1]\n",
    "            df['fname'][str(i)] = audio\n",
    "            \n",
    "            if ext is not 'md' or ext is not 'MD':\n",
    "                rate, clip = scipy.io.wavfile.read(TEST_DIR + audio)\n",
    "                if clip.shape[0] < 16000:\n",
    "                    clip = np.pad(clip, (0, 16000 - len(clip)), 'constant')\n",
    "                \n",
    "                if clip.shape[0] > 16000:                            \n",
    "                    print('Found a clip in TEST DIR with shape more than 16000')\n",
    "                    print(clip.shape)\n",
    "                \n",
    "                test.append(clip)\n",
    "        \n",
    "        test = np.array(test)\n",
    "        np.save('test.npy', test)\n",
    "        \n",
    "        df.to_csv('predictions.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 969/158538 [00:00<00:16, 9679.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158538/158538 [04:55<00:00, 536.81it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./train.npy') == False or os.path.isfile('./labels.npy') == False:\n",
    "    read_data(train, labels, test)\n",
    "else:\n",
    "    train = np.load('train.npy')\n",
    "    labels = np.load('labels.npy')\n",
    "    \n",
    "if os.path.isfile('./test.npy') == False:\n",
    "    read_data(train, labels, test, is_testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62987, 16000)\n",
      "(62987, 11)\n"
     ]
    }
   ],
   "source": [
    "# np.set_printoptions(threshold=np.nan)\n",
    "train = np.load('train.npy')\n",
    "print(train.shape)\n",
    "\n",
    "labels = np.load('labels.npy')\n",
    "print(labels.shape)\n",
    "# train[0].shape\n",
    "# print(train[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "The neural network needs to read the audio data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement neural_net_input\n",
    "    * Return a TF Placeholder\n",
    "    * Set the shape using `shape` with batch size set to None.\n",
    "    * Name the TensorFlow placeholder \"x\" using the TensorFlow name parameter in the TF Placeholder.\n",
    "* Implement neural_net_label\n",
    "    * Return a TF Placeholder\n",
    "    * Set the shape using n_classes with batch size set to None.\n",
    "    * Name the TensorFlow placeholder \"y\" using the TensorFlow name parameter in the TF Placeholder.\n",
    "* Implement neural_net_keep_prob_input\n",
    "    * Return a TF Placeholder for dropout keep probability.\n",
    "    * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow name parameter in the TF Placeholder.\n",
    "    * These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: _None_ for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_input(shape):\n",
    "    return tf.placeholder(tf.float32, [None, shape[1]], 'x')\n",
    "\n",
    "def neural_net_label(n_classes=11):\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], 'y')\n",
    "\n",
    "def neural_net_keep_prob():\n",
    "    return tf.placeholder(tf.float32, None, 'keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "Implement the input_layer function whose output should be a layer with ___16000___ neurons. Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_layer(x_tensor, n_neurons, keep_prob):\n",
    "    features_count=16000\n",
    "    weight = tf.Variable(tf.truncated_normal((features_count, n_neurons), 0, 0.1))\n",
    "    bias = tf.Variable(tf.zeros(n_neurons))\n",
    "    \n",
    "    model = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    model = tf.nn.relu(model)\n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to create a fully connected layer which is also the hidden layer. Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)] packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, n_neurons, keep_prob):\n",
    "    x_dim = x_tensor.get_shape().as_list()\n",
    "    \n",
    "    weight = tf.Variable(tf.truncated_normal(list((x_dim[1],) + (n_neurons,)), 0, 0.1))\n",
    "    bias = tf.Variable(tf.zeros(n_neurons))\n",
    "    \n",
    "    model = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    model = tf.nn.relu(model)\n",
    "    model = tf.nn.dropout(model, keep_prob)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the output function to create the final output layer. Since we need to classify ___11___ labels, this layer will have 11 neurons. Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "__Note__: Activation, softmax, or cross entropy should __not__ be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_layer(x_tensor, n_classes=11):\n",
    "    x_dim = x_tensor.get_shape().as_list()\n",
    "    weight = tf.Variable(tf.truncated_normal(list((x_dim[1],) + (n_classes,)), 0, 0.01))\n",
    "    bias = tf.Variable(tf.zeros(n_classes))\n",
    "    \n",
    "    output = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, x, and outputs logits. Use the layers you created above to create this model:\n",
    "* Create one input layer\n",
    "* Apply 1, 2, or 3 hidden layers\n",
    "* Apply an Output Layer\n",
    "* Return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net(x, y, keep_prob):\n",
    "    model = input_layer(x, 128, keep_prob)\n",
    "#     model = fully_conn(model, 128, keep_prob)\n",
    "    model = fully_conn(model, 64, keep_prob)\n",
    "    model = fully_conn(model, 32, keep_prob)\n",
    "    model = fully_conn(model, 16, keep_prob)\n",
    "    model = output_layer(model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_input(train.shape)\n",
    "y = neural_net_label()\n",
    "keep_prob = neural_net_keep_prob()\n",
    "\n",
    "# Model\n",
    "logits = neural_net(x, y, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimization\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization. The optimization should use `optimizer` to optimize in session with a `feed_dict` of the following:\n",
    "* x for image input\n",
    "* y for labels\n",
    "* keep_prob for keep probability for dropout\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, feed_dict={x:feature_batch, y:label_batch, keep_prob:keep_probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy. Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy. Use a keep probability of 1.0 to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy, val_features, val_labels):\n",
    "    loss = session.run(cost, feed_dict={x:feature_batch, y:label_batch, keep_prob:1.0})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "        x: val_features,\n",
    "        y: val_labels,\n",
    "        keep_prob: 1.0\n",
    "    })\n",
    "    print('Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set epochs to the number of iterations until the network stops learning or start overfitting\n",
    "* Set batch_size to the highest number that your machine has memory for. Most people set them to common sizes of memory:\n",
    "    * 64\n",
    "    * 128\n",
    "    * 256\n",
    "    * ...\n",
    "* Set keep_probability to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a Single Batch\n",
    "Instead of training the neural network on all the batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy. Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        i = 0\n",
    "        index = batch_size\n",
    "\n",
    "        batch_features = train[i:index]\n",
    "        batch_labels = labels[i:index]\n",
    "            \n",
    "        batch_features, val_features, batch_labels, val_labels = train_test_split(batch_features, \n",
    "                                                                                     batch_labels, test_size=0.1)\n",
    "        train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)    \n",
    "        print('Epoch {:>2}, Speech Batch {}:  '.format(epoch + 1, 0), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Train the Model\n",
    "Now that you got a good accuracy with a single batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, Speech Batch 1:  Loss:    25.2190 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 100:  Loss:     0.1317 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 200:  Loss:     1.5928 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 300:  Loss:     2.5106 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 400:  Loss:     2.3795 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 500:  Loss:     2.5137 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 600:  Loss:     2.6212 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 700:  Loss:     2.7324 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 800:  Loss:     2.7323 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 900:  Loss:     2.4750 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 984:  Loss:     2.3052 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 1000:  Loss:     2.2755 Validation Accuracy: 0.000000\n",
      "Epoch  1, Speech Batch 1100:  Loss:     2.1010 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1200:  Loss:     1.9424 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1300:  Loss:     1.7957 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1400:  Loss:     1.6584 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1500:  Loss:     1.5298 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1600:  Loss:     1.4096 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1700:  Loss:     1.2968 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1800:  Loss:     1.1913 Validation Accuracy: 1.000000\n",
      "Epoch  1, Speech Batch 1900:  Loss:     1.0931 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1:  Loss:     3.0196 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 100:  Loss:     2.8077 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 200:  Loss:     2.6393 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 300:  Loss:     2.7498 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 400:  Loss:     2.6367 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 500:  Loss:     2.6611 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 600:  Loss:     2.8425 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 700:  Loss:     2.8468 Validation Accuracy: 0.000000\n",
      "Epoch  2, Speech Batch 800:  Loss:     1.2609 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 900:  Loss:     1.1495 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 984:  Loss:     1.0650 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1000:  Loss:     1.0498 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1100:  Loss:     0.9599 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1200:  Loss:     0.8786 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1300:  Loss:     0.8052 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1400:  Loss:     0.7386 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1500:  Loss:     0.6782 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1600:  Loss:     0.6233 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1700:  Loss:     0.5736 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1800:  Loss:     0.5283 Validation Accuracy: 1.000000\n",
      "Epoch  2, Speech Batch 1900:  Loss:     0.4872 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1:  Loss:     3.4781 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 100:  Loss:     3.2658 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 200:  Loss:     3.0670 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 300:  Loss:     3.1748 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 400:  Loss:     3.0173 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 500:  Loss:     2.9523 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 600:  Loss:     3.1225 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 700:  Loss:     3.0539 Validation Accuracy: 0.000000\n",
      "Epoch  3, Speech Batch 800:  Loss:     0.8321 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 900:  Loss:     0.7605 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 984:  Loss:     0.7062 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1000:  Loss:     0.6964 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1100:  Loss:     0.6389 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1200:  Loss:     0.5872 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1300:  Loss:     0.5406 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1400:  Loss:     0.4983 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1500:  Loss:     0.4601 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1600:  Loss:     0.4255 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1700:  Loss:     0.3939 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1800:  Loss:     0.3652 Validation Accuracy: 1.000000\n",
      "Epoch  3, Speech Batch 1900:  Loss:     0.3391 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1:  Loss:     3.7290 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 100:  Loss:     3.4979 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 200:  Loss:     3.2765 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 300:  Loss:     3.3818 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 400:  Loss:     3.2011 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 500:  Loss:     3.0914 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 600:  Loss:     3.2538 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 700:  Loss:     3.1494 Validation Accuracy: 0.000000\n",
      "Epoch  4, Speech Batch 800:  Loss:     0.7083 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 900:  Loss:     0.6502 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 984:  Loss:     0.6060 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1000:  Loss:     0.5980 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1100:  Loss:     0.5508 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1200:  Loss:     0.5082 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1300:  Loss:     0.4696 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1400:  Loss:     0.4345 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1500:  Loss:     0.4026 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1600:  Loss:     0.3735 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1700:  Loss:     0.3470 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1800:  Loss:     0.3228 Validation Accuracy: 1.000000\n",
      "Epoch  4, Speech Batch 1900:  Loss:     0.3006 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1:  Loss:     3.8075 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 100:  Loss:     3.5736 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 200:  Loss:     3.3478 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 300:  Loss:     3.4587 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 400:  Loss:     3.2716 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 500:  Loss:     3.1403 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 600:  Loss:     3.3025 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 700:  Loss:     3.1810 Validation Accuracy: 0.000000\n",
      "Epoch  5, Speech Batch 800:  Loss:     0.6701 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 900:  Loss:     0.6166 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 984:  Loss:     0.5756 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1000:  Loss:     0.5682 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1100:  Loss:     0.5244 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1200:  Loss:     0.4847 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1300:  Loss:     0.4486 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1400:  Loss:     0.4157 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1500:  Loss:     0.3858 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1600:  Loss:     0.3585 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1700:  Loss:     0.0000 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1800:  Loss:     0.0000 Validation Accuracy: 1.000000\n",
      "Epoch  5, Speech Batch 1900:  Loss:     0.0000 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1:  Loss:  1043.4189 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 100:  Loss:     3.4814 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 200:  Loss:     3.2621 Validation Accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6, Speech Batch 300:  Loss:     3.3896 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 400:  Loss:     3.2092 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 500:  Loss:     3.0739 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 600:  Loss:     3.2460 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 700:  Loss:     3.1219 Validation Accuracy: 0.000000\n",
      "Epoch  6, Speech Batch 800:  Loss:     0.7245 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 900:  Loss:     0.6626 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 984:  Loss:     0.6158 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1000:  Loss:     0.6074 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1100:  Loss:     0.5579 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1200:  Loss:     0.5134 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1300:  Loss:     0.4733 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1400:  Loss:     0.4371 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1500:  Loss:     0.4043 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1600:  Loss:     0.3745 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1700:  Loss:     0.3474 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1800:  Loss:     0.3227 Validation Accuracy: 1.000000\n",
      "Epoch  6, Speech Batch 1900:  Loss:     0.3002 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1:  Loss:     3.8053 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 100:  Loss:     3.5610 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 200:  Loss:     3.3395 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 300:  Loss:     3.4667 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 400:  Loss:     3.2822 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 500:  Loss:     3.1353 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 600:  Loss:     3.3042 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 700:  Loss:     3.1700 Validation Accuracy: 0.000000\n",
      "Epoch  7, Speech Batch 800:  Loss:     0.6726 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 900:  Loss:     0.6185 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 984:  Loss:     0.5772 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1000:  Loss:     0.5697 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1100:  Loss:     0.5256 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1200:  Loss:     0.4856 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1300:  Loss:     0.4493 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1400:  Loss:     0.4163 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1500:  Loss:     0.3862 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1600:  Loss:     0.3587 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1700:  Loss:     0.3336 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1800:  Loss:     0.3106 Validation Accuracy: 1.000000\n",
      "Epoch  7, Speech Batch 1900:  Loss:     0.2895 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1:  Loss:     3.8221 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 100:  Loss:     3.5859 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 200:  Loss:     3.3652 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 300:  Loss:     3.4937 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 400:  Loss:     3.3073 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 500:  Loss:     3.1541 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 600:  Loss:     3.3225 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 700:  Loss:     3.1825 Validation Accuracy: 0.000000\n",
      "Epoch  8, Speech Batch 800:  Loss:     0.6583 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 900:  Loss:     0.6061 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 984:  Loss:     0.5662 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1000:  Loss:     0.5589 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1100:  Loss:     0.5162 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1200:  Loss:     0.4774 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1300:  Loss:     0.4421 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1400:  Loss:     0.4099 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1500:  Loss:     0.3806 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1600:  Loss:     0.3538 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1700:  Loss:     0.3292 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1800:  Loss:     0.3067 Validation Accuracy: 1.000000\n",
      "Epoch  8, Speech Batch 1900:  Loss:     0.2861 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1:  Loss:     3.8238 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 100:  Loss:     3.5929 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 200:  Loss:     3.3737 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 300:  Loss:     3.5039 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 400:  Loss:     3.3168 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 500:  Loss:     3.1598 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 600:  Loss:     3.3287 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 700:  Loss:     3.1853 Validation Accuracy: 0.000000\n",
      "Epoch  9, Speech Batch 800:  Loss:     0.6540 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 900:  Loss:     0.6024 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 984:  Loss:     0.5629 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1000:  Loss:     0.5557 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1100:  Loss:     0.5134 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1200:  Loss:     0.4749 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1300:  Loss:     0.4399 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1400:  Loss:     0.4080 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1500:  Loss:     0.3789 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1600:  Loss:     0.3522 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1700:  Loss:     0.3279 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1800:  Loss:     0.3055 Validation Accuracy: 1.000000\n",
      "Epoch  9, Speech Batch 1900:  Loss:     0.2850 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1:  Loss:     3.8213 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 100:  Loss:     3.5947 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 200:  Loss:     3.3765 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 300:  Loss:     3.5084 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 400:  Loss:     3.3210 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 500:  Loss:     3.1621 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 600:  Loss:     3.3310 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 700:  Loss:     3.1858 Validation Accuracy: 0.000000\n",
      "Epoch 10, Speech Batch 800:  Loss:     0.6527 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 900:  Loss:     0.6012 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 984:  Loss:     0.5618 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1000:  Loss:     0.5547 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1100:  Loss:     0.5124 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1200:  Loss:     0.4741 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1300:  Loss:     0.4392 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1400:  Loss:     0.4074 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1500:  Loss:     0.3783 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1600:  Loss:     0.3517 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1700:  Loss:     0.3274 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1800:  Loss:     0.3051 Validation Accuracy: 1.000000\n",
      "Epoch 10, Speech Batch 1900:  Loss:     0.2846 Validation Accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './model'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    n_batches = train.shape[0] // batch_size        \n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        i = 0\n",
    "        index = batch_size        \n",
    "        # Loop over all batches        \n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            batch_features = train[i:index]\n",
    "            batch_labels = labels[i:index]\n",
    "            \n",
    "            batch_features, val_features, batch_labels, val_labels = train_test_split(batch_features, \n",
    "                                                                                     batch_labels, test_size=0.1)\n",
    "            \n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            if batch_i == 1 or batch_i % 100 == 0 or batch_i == 984:\n",
    "                print('Epoch {:>2}, Speech Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "                print_stats(sess, batch_features, batch_labels, cost, accuracy, val_features, val_labels)\n",
    "\n",
    "            i += batch_size\n",
    "            index += batch_size\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset. This will be your final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4cb7b37aac9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclip\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpredict_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_input_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#         predicted_classes = [p[\"classes\"] for p in predictions]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         predictions = sess.run([predictor], feed_dict={x: clip})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "    \n",
    "    loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "    predictor = tf.argmax(loaded_y,1)\n",
    "    \n",
    "    test = np.load('test.npy')\n",
    "    \n",
    "    for clip in test:\n",
    "        predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x\": clip}, num_epochs=1, shuffle=False)\n",
    "        predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "#         predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "#         predictions = sess.run([predictor], feed_dict={x: clip})\n",
    "        print(predictions)\n",
    "#         print(sess.run(y, feed_dict={x: clip}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
